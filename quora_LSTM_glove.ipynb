{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import tensorflow\n",
    "import gc\n",
    "\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del classifier # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    \n",
    "reset_keras()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.regularizers import L1L2\n",
    "import keras.backend as K\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "\n",
    "print(\"Train shape : \",train_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config values \n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=2019)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.5, random_state=2020)\n",
    "print(\"Validation shape : \",val_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 70000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 50 # max number of words in a question to use\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "val_df[\"question_text\"] = val_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "## fill up the missing values\n",
    "train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "val_X = val_df[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test_df[\"question_text\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "oov_tok = \"OOV_TOK\"\n",
    "tokenizer = Tokenizer(num_words=max_features) # lower = False, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen, truncating='post')\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen, truncating='post')\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen, truncating='post')\n",
    "\n",
    "## Get the target values\n",
    "train_y = train_df['target'].values\n",
    "val_y = val_df['target'].values\n",
    "test_y = test_df['target'].values\n",
    "del train_df, val_df, test_df\n",
    "gc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer = L1L2(l1=0.0, l2=0.001)\n",
    "def get_model():\n",
    "    K.clear_session() \n",
    "    inp = Input(shape=(maxlen,))\n",
    "    emb_out = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    lstm_out = Bidirectional(CuDNNLSTM(64, return_sequences=True))(emb_out)\n",
    "    gmp_out = GlobalMaxPool1D()(lstm_out)\n",
    "    d_out = Dense(16, activation=\"relu\")(gmp_out)\n",
    "    drop_out = Dropout(0.1)(d_out)\n",
    "    d_out = Dense(1, activation=\"sigmoid\")(drop_out)\n",
    "    model = Model(inputs=inp, outputs=d_out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return(model)\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function if detailed log required\n",
    "\n",
    "class History(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.val_loss = []\n",
    "        for keys in logs:\n",
    "           print(keys) \n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.train_acc.append(logs.get('acc'))\n",
    "    #Store val_acc/loss per batch    \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        \n",
    "# Compute class_weights for imbalanced train set\n",
    "def compute_class_weight(input_list):\n",
    "\n",
    "  class_weights = class_weight.compute_class_weight('balanced', \n",
    "                                                   np.unique(input_list),  \n",
    "                                                   input_list)\n",
    "                                                   \n",
    "  return(class_weights)                                                   \n",
    "        \n",
    "#define callback functions\n",
    "history = History()\n",
    "callbacks = [history]        \n",
    "\n",
    "class_weights = compute_class_weight(train_y)\n",
    "\n",
    "class_weights[1] *= 10\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(train_X, train_y, \n",
    "          batch_size=512, \n",
    "          epochs=2, \n",
    "          validation_data=(val_X, val_y), \n",
    "          class_weight = class_weights,\n",
    "          callbacks= callbacks)\n",
    "model.save_weights('./output/model_glove_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "#model.load_weights('./output/model_glove_weights.h5')\n",
    "pred_glove_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "threshold = []\n",
    "f1_array = []\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    threshold.append(thresh)\n",
    "    f1_score = metrics.f1_score(val_y, (pred_glove_val_y>thresh).astype(int))\n",
    "    f1_array.append(f1_score)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = range(len(history.train_acc))\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "plt.xlabel('Batch Count')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.plot(x[0:1000], history.train_acc[0:1000])\n",
    "plt.show()\n",
    "#fig.savefig('glove_emb_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.xlabel('Batch Count')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.plot(x[0:1000], history.train_loss[0:1000])\n",
    "plt.show()\n",
    "#fig.savefig('glove_emb_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.plot(threshold, f1_array)\n",
    "plt.show()\n",
    "#fig.savefig('self_train_emb_F1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_y = model.predict([test_X], batch_size=1024, verbose=1)\n",
    "threshold = []\n",
    "f1_array = []\n",
    "f1_max = 0\n",
    "opt_thresh = 0\n",
    "test_y = test_df['target'].values\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    threshold.append(thresh)\n",
    "    f1_score = metrics.f1_score(test_y, (pred_test_y>thresh).astype(int))\n",
    "    if(f1_score > f1_max):\n",
    "        f1_max = f1_score\n",
    "        opt_thresh = thresh\n",
    "    f1_array.append(f1_score)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, f1_score))\n",
    "\n",
    "pred = [int(a > opt_thresh) for a in pred_test_y]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.plot(threshold, f1_array)\n",
    "plt.show()\n",
    "#fig.savefig('glove_emb_F1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import scikitplot as skplt\n",
    "import seaborn as sns\n",
    "def get_confusion_matrix(val_y, pred,title):\n",
    "    cm=confusion_matrix(pred,val_y)\n",
    "   \n",
    "    fig = plt.figure()\n",
    "    sns.heatmap(cm, cmap='coolwarm', annot=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('True labels')\n",
    "    plt.ylabel('Predicted labels')\n",
    "    plt.show()\n",
    "    #fig.savefig('glove_confusion_matrix')\n",
    " \n",
    "get_confusion_matrix(test_y,pred,'Confusion_matrix Glove')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(test_y, pred_test_y)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "fig = plt.figure()\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "#fig.savefig('glove_ROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
