{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [02:00, 18154.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emb_index = {}\n",
    "emb_file = open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n",
    "for line in tqdm(emb_file):\n",
    "    emb_values = line.split(\" \")\n",
    "    word = emb_values[0]\n",
    "    vecs = np.asarray(emb_values[1:], dtype='float32')\n",
    "    emb_index[word] = vecs\n",
    "emb_file.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(emb_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP = 30\n",
    "EMB_VEC_SIZE = 300\n",
    "BATCH_SIZE = 128\n",
    "def text_to_vec(text):\n",
    "    init_emb = np.zeros(EMB_VEC_SIZE)\n",
    "    text = text[:-1].split()[:TIME_STEP]\n",
    "    embeds = [emb_index.get(x, init_emb) for x in text]\n",
    "    embeds+= [init_emb] * (TIME_STEP - len(embeds))\n",
    "    return np.array(embeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_gen(train_df, shuffle=False):\n",
    "    n_batches = math.ceil(len(train_df) / BATCH_SIZE)\n",
    "    while True: \n",
    "        if(shuffle==True):\n",
    "            train_df = train_df.sample(frac=1.)  # Shuffle the data.\n",
    "        for i in range(n_batches):\n",
    "            texts = train_df.iloc[i*BATCH_SIZE:(i+1)*BATCH_SIZE, 1]\n",
    "            text_arr = np.array([text_to_vec(text) for text in texts])\n",
    "            yield text_arr, np.array(train_df[\"target\"][i*BATCH_SIZE:(i+1)*BATCH_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import  Model\n",
    "from keras.layers import LSTM, Dense, Bidirectional, Input, CuDNNLSTM\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_gpus():\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "    \n",
    "def get_base_model(input):\n",
    "\n",
    "  num_gpus = get_available_gpus()\n",
    "  #print(len(num_gpus))\n",
    "  #Build LSTM network\n",
    "\n",
    "  if(len(num_gpus)>0):\n",
    "    first_lstm = Bidirectional(CuDNNLSTM(64, return_sequences=True, name='lstm1'))(input)\n",
    "    second_lstm = Bidirectional(CuDNNLSTM(64, name='lstm2'))(first_lstm)\n",
    "  else:  \n",
    "    first_lstm = Bidirectional(LSTM(64, return_sequences=True, name='lstm1'))(input)\n",
    "    second_lstm = Bidirectional(LSTM(64, name='lstm2'))(first_lstm)\n",
    "  outputs = Dense(1, activation=\"sigmoid\", name='last')(second_lstm)\n",
    "  \n",
    "  #create model LSTM+dense\n",
    "  l_model = Model(input, outputs) \n",
    "  \n",
    "  return(l_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "features = Input(shape=(TIME_STEP, EMB_VEC_SIZE))\n",
    "model = get_base_model(features)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "500/500 [==============================] - 167s 333ms/step - loss: 0.1098 - acc: 0.9568 - val_loss: 0.1090 - val_acc: 0.9570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ac3e246a0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_generator = train_gen(train_df, True)\n",
    "validation_generator = train_gen(val_df, False)\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=int(len(train_df) / BATCH_SIZE)\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps = int(len(val_df) / BATCH_SIZE)\n",
    "                    epochs=20,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
